# Cross-validation
1)To compare and verify which model is better in which situation. 2) Clean the data, Hyper Tuning both models, Building function to evaluate the models, training validating the model and comparing, then applying K-fold to see the improvement. 3) It's been showed how LightGbm is faster than XGboost, however, XGboost works better with lower data than LightGbm as it trains better, and when applying K-fold it increases the accuracy more and more.
